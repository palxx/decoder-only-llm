{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "162156fa-5bec-4650-822c-6ee448050d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 60000\n",
    "learning_rate = 3e-4\n",
    "eval_iters=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1563ce-4ca0-4365-aadb-d50f2da1dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard-of-oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792c814e-d900-49db-8823-0d88587a277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17f8ea9-68ec-4dd7-ac4e-be11a9f49c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2faecad-070b-46f9-82fb-4dbc64fc0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SA8L)D48bCQyZzP7oU*felD4eEc7Qgp\n",
      "??,m 4dA2CIk﻿Xv cAnt,GMEGi:VB7A6VxP.Kj&)-M0nI0Y]?mnMdSPBlEaU_1?(tv.vcl(pbyq.paa6Bw]U*NMKK0pwa1ijE5UWHl'7G﻿H4OWjDZfvZrBfQ)AK7xJ6\"j9xTza-XeuRueu'Z']VS8L-kq\"R.GR&wIQ:owEO4'h?.7[qFdcrD4'ufk4osE04BxX6\"go8D4kT49kZD4-i799\n",
      "m&,Y!OZ]qR)Lr'4OMAlE N&83k﻿[SL-1Z[z]sa-l\".:tyHj9-9-bgNvNkT.RuPPPD4kxE MKK5NNManMI,hB5?dz2[1tmAtjZG*!YU_v)OxdLXF*6W2Y8HshihSmWVdO3:N.H5fcAi5-](HQbYLdzCfeuRniG-3MEOiDX]49UWRhzyP9HJqo;R*'O49NL,qm.*tmaVBApGQL.qiazkMzj﻿VjS;3Z-A ﻿wW2D4j2BO]DWR*cr'5CmAYCl)_E&D\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples =1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "\n",
    "        #print(index)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a8366b1-fb6d-4fc5-8605-430cc8f4c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out ={}\n",
    "    model.eval()\n",
    "    for tt in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(tt)\n",
    "            logits, loss=model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[tt] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "741a247c-064b-46cd-a454-09fcff953342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, {'train': tensor(2.4463), 'test': tensor(2.4688)}\n",
      "step:250, {'train': tensor(2.4237), 'test': tensor(2.4748)}\n",
      "step:500, {'train': tensor(2.4194), 'test': tensor(2.4849)}\n",
      "step:750, {'train': tensor(2.4173), 'test': tensor(2.4669)}\n",
      "step:1000, {'train': tensor(2.4249), 'test': tensor(2.4591)}\n",
      "step:1250, {'train': tensor(2.4270), 'test': tensor(2.4418)}\n",
      "step:1500, {'train': tensor(2.4413), 'test': tensor(2.4747)}\n",
      "step:1750, {'train': tensor(2.4335), 'test': tensor(2.4745)}\n",
      "step:2000, {'train': tensor(2.4259), 'test': tensor(2.4811)}\n",
      "step:2250, {'train': tensor(2.4242), 'test': tensor(2.4935)}\n",
      "step:2500, {'train': tensor(2.4268), 'test': tensor(2.4659)}\n",
      "step:2750, {'train': tensor(2.4238), 'test': tensor(2.4802)}\n",
      "step:3000, {'train': tensor(2.4192), 'test': tensor(2.4803)}\n",
      "step:3250, {'train': tensor(2.4297), 'test': tensor(2.4793)}\n",
      "step:3500, {'train': tensor(2.4382), 'test': tensor(2.4537)}\n",
      "step:3750, {'train': tensor(2.4261), 'test': tensor(2.4818)}\n",
      "step:4000, {'train': tensor(2.4323), 'test': tensor(2.4771)}\n",
      "step:4250, {'train': tensor(2.4154), 'test': tensor(2.4720)}\n",
      "step:4500, {'train': tensor(2.4235), 'test': tensor(2.4745)}\n",
      "step:4750, {'train': tensor(2.4355), 'test': tensor(2.4813)}\n",
      "step:5000, {'train': tensor(2.4219), 'test': tensor(2.5113)}\n",
      "step:5250, {'train': tensor(2.4389), 'test': tensor(2.4811)}\n",
      "step:5500, {'train': tensor(2.4311), 'test': tensor(2.4635)}\n",
      "step:5750, {'train': tensor(2.4352), 'test': tensor(2.4653)}\n",
      "step:6000, {'train': tensor(2.4340), 'test': tensor(2.4762)}\n",
      "step:6250, {'train': tensor(2.4219), 'test': tensor(2.5032)}\n",
      "step:6500, {'train': tensor(2.4392), 'test': tensor(2.4877)}\n",
      "step:6750, {'train': tensor(2.4283), 'test': tensor(2.4689)}\n",
      "step:7000, {'train': tensor(2.4424), 'test': tensor(2.4630)}\n",
      "step:7250, {'train': tensor(2.4130), 'test': tensor(2.4666)}\n",
      "step:7500, {'train': tensor(2.4214), 'test': tensor(2.4854)}\n",
      "step:7750, {'train': tensor(2.4133), 'test': tensor(2.4701)}\n",
      "step:8000, {'train': tensor(2.4225), 'test': tensor(2.4830)}\n",
      "step:8250, {'train': tensor(2.4275), 'test': tensor(2.4733)}\n",
      "step:8500, {'train': tensor(2.4356), 'test': tensor(2.4425)}\n",
      "step:8750, {'train': tensor(2.4121), 'test': tensor(2.4681)}\n",
      "step:9000, {'train': tensor(2.4200), 'test': tensor(2.4831)}\n",
      "step:9250, {'train': tensor(2.4113), 'test': tensor(2.4556)}\n",
      "step:9500, {'train': tensor(2.4339), 'test': tensor(2.4515)}\n",
      "step:9750, {'train': tensor(2.4505), 'test': tensor(2.4550)}\n",
      "step:10000, {'train': tensor(2.4242), 'test': tensor(2.4641)}\n",
      "step:10250, {'train': tensor(2.4140), 'test': tensor(2.4616)}\n",
      "step:10500, {'train': tensor(2.4211), 'test': tensor(2.4795)}\n",
      "step:10750, {'train': tensor(2.4017), 'test': tensor(2.4595)}\n",
      "step:11000, {'train': tensor(2.4290), 'test': tensor(2.4862)}\n",
      "step:11250, {'train': tensor(2.4457), 'test': tensor(2.4793)}\n",
      "step:11500, {'train': tensor(2.4339), 'test': tensor(2.4838)}\n",
      "step:11750, {'train': tensor(2.4330), 'test': tensor(2.5055)}\n",
      "step:12000, {'train': tensor(2.4011), 'test': tensor(2.4727)}\n",
      "step:12250, {'train': tensor(2.4228), 'test': tensor(2.4877)}\n",
      "step:12500, {'train': tensor(2.4236), 'test': tensor(2.4870)}\n",
      "step:12750, {'train': tensor(2.4303), 'test': tensor(2.4632)}\n",
      "step:13000, {'train': tensor(2.4438), 'test': tensor(2.4751)}\n",
      "step:13250, {'train': tensor(2.4463), 'test': tensor(2.4505)}\n",
      "step:13500, {'train': tensor(2.4154), 'test': tensor(2.4767)}\n",
      "step:13750, {'train': tensor(2.4135), 'test': tensor(2.4479)}\n",
      "step:14000, {'train': tensor(2.4249), 'test': tensor(2.4676)}\n",
      "step:14250, {'train': tensor(2.4134), 'test': tensor(2.4521)}\n",
      "step:14500, {'train': tensor(2.4520), 'test': tensor(2.4714)}\n",
      "step:14750, {'train': tensor(2.4150), 'test': tensor(2.4854)}\n",
      "step:15000, {'train': tensor(2.4110), 'test': tensor(2.4769)}\n",
      "step:15250, {'train': tensor(2.4269), 'test': tensor(2.5036)}\n",
      "step:15500, {'train': tensor(2.4374), 'test': tensor(2.4512)}\n",
      "step:15750, {'train': tensor(2.4158), 'test': tensor(2.4698)}\n",
      "step:16000, {'train': tensor(2.4357), 'test': tensor(2.4752)}\n",
      "step:16250, {'train': tensor(2.4444), 'test': tensor(2.4522)}\n",
      "step:16500, {'train': tensor(2.4269), 'test': tensor(2.4872)}\n",
      "step:16750, {'train': tensor(2.4110), 'test': tensor(2.4511)}\n",
      "step:17000, {'train': tensor(2.4150), 'test': tensor(2.4966)}\n",
      "step:17250, {'train': tensor(2.4531), 'test': tensor(2.4636)}\n",
      "step:17500, {'train': tensor(2.4130), 'test': tensor(2.4485)}\n",
      "step:17750, {'train': tensor(2.4510), 'test': tensor(2.4787)}\n",
      "step:18000, {'train': tensor(2.4246), 'test': tensor(2.4634)}\n",
      "step:18250, {'train': tensor(2.4481), 'test': tensor(2.4892)}\n",
      "step:18500, {'train': tensor(2.4079), 'test': tensor(2.4961)}\n",
      "step:18750, {'train': tensor(2.4156), 'test': tensor(2.4863)}\n",
      "step:19000, {'train': tensor(2.4349), 'test': tensor(2.4848)}\n",
      "step:19250, {'train': tensor(2.4163), 'test': tensor(2.4434)}\n",
      "step:19500, {'train': tensor(2.4368), 'test': tensor(2.4768)}\n",
      "step:19750, {'train': tensor(2.4366), 'test': tensor(2.4659)}\n",
      "step:20000, {'train': tensor(2.4090), 'test': tensor(2.4850)}\n",
      "step:20250, {'train': tensor(2.4023), 'test': tensor(2.4808)}\n",
      "step:20500, {'train': tensor(2.4338), 'test': tensor(2.5009)}\n",
      "step:20750, {'train': tensor(2.4260), 'test': tensor(2.4672)}\n",
      "step:21000, {'train': tensor(2.4237), 'test': tensor(2.4499)}\n",
      "step:21250, {'train': tensor(2.4033), 'test': tensor(2.4802)}\n",
      "step:21500, {'train': tensor(2.4449), 'test': tensor(2.5064)}\n",
      "step:21750, {'train': tensor(2.4106), 'test': tensor(2.4904)}\n",
      "step:22000, {'train': tensor(2.4490), 'test': tensor(2.4921)}\n",
      "step:22250, {'train': tensor(2.4067), 'test': tensor(2.4795)}\n",
      "step:22500, {'train': tensor(2.4360), 'test': tensor(2.4778)}\n",
      "step:22750, {'train': tensor(2.4582), 'test': tensor(2.4613)}\n",
      "step:23000, {'train': tensor(2.4107), 'test': tensor(2.4607)}\n",
      "step:23250, {'train': tensor(2.4174), 'test': tensor(2.4872)}\n",
      "step:23500, {'train': tensor(2.4157), 'test': tensor(2.4854)}\n",
      "step:23750, {'train': tensor(2.4276), 'test': tensor(2.4855)}\n",
      "step:24000, {'train': tensor(2.4449), 'test': tensor(2.4613)}\n",
      "step:24250, {'train': tensor(2.4240), 'test': tensor(2.4910)}\n",
      "step:24500, {'train': tensor(2.4458), 'test': tensor(2.4822)}\n",
      "step:24750, {'train': tensor(2.3970), 'test': tensor(2.4484)}\n",
      "step:25000, {'train': tensor(2.4515), 'test': tensor(2.4726)}\n",
      "step:25250, {'train': tensor(2.4072), 'test': tensor(2.4739)}\n",
      "step:25500, {'train': tensor(2.4292), 'test': tensor(2.4825)}\n",
      "step:25750, {'train': tensor(2.4312), 'test': tensor(2.4805)}\n",
      "step:26000, {'train': tensor(2.4064), 'test': tensor(2.4954)}\n",
      "step:26250, {'train': tensor(2.4111), 'test': tensor(2.4931)}\n",
      "step:26500, {'train': tensor(2.4134), 'test': tensor(2.4658)}\n",
      "step:26750, {'train': tensor(2.4209), 'test': tensor(2.4659)}\n",
      "step:27000, {'train': tensor(2.3753), 'test': tensor(2.4481)}\n",
      "step:27250, {'train': tensor(2.4200), 'test': tensor(2.4848)}\n",
      "step:27500, {'train': tensor(2.4038), 'test': tensor(2.4963)}\n",
      "step:27750, {'train': tensor(2.4166), 'test': tensor(2.4565)}\n",
      "step:28000, {'train': tensor(2.4432), 'test': tensor(2.4865)}\n",
      "step:28250, {'train': tensor(2.4383), 'test': tensor(2.4641)}\n",
      "step:28500, {'train': tensor(2.4275), 'test': tensor(2.4666)}\n",
      "step:28750, {'train': tensor(2.4390), 'test': tensor(2.4636)}\n",
      "step:29000, {'train': tensor(2.3977), 'test': tensor(2.4757)}\n",
      "step:29250, {'train': tensor(2.4208), 'test': tensor(2.4776)}\n",
      "step:29500, {'train': tensor(2.4147), 'test': tensor(2.4690)}\n",
      "step:29750, {'train': tensor(2.4265), 'test': tensor(2.4680)}\n",
      "step:30000, {'train': tensor(2.4195), 'test': tensor(2.4816)}\n",
      "step:30250, {'train': tensor(2.4368), 'test': tensor(2.4521)}\n",
      "step:30500, {'train': tensor(2.4207), 'test': tensor(2.4771)}\n",
      "step:30750, {'train': tensor(2.4457), 'test': tensor(2.4643)}\n",
      "step:31000, {'train': tensor(2.4073), 'test': tensor(2.4568)}\n",
      "step:31250, {'train': tensor(2.4228), 'test': tensor(2.4691)}\n",
      "step:31500, {'train': tensor(2.4600), 'test': tensor(2.4856)}\n",
      "step:31750, {'train': tensor(2.4002), 'test': tensor(2.4823)}\n",
      "step:32000, {'train': tensor(2.4375), 'test': tensor(2.4711)}\n",
      "step:32250, {'train': tensor(2.4337), 'test': tensor(2.4452)}\n",
      "step:32500, {'train': tensor(2.4225), 'test': tensor(2.4504)}\n",
      "step:32750, {'train': tensor(2.4490), 'test': tensor(2.4690)}\n",
      "step:33000, {'train': tensor(2.4601), 'test': tensor(2.4740)}\n",
      "step:33250, {'train': tensor(2.4210), 'test': tensor(2.4810)}\n",
      "step:33500, {'train': tensor(2.4276), 'test': tensor(2.4965)}\n",
      "step:33750, {'train': tensor(2.4379), 'test': tensor(2.4840)}\n",
      "step:34000, {'train': tensor(2.4303), 'test': tensor(2.4706)}\n",
      "step:34250, {'train': tensor(2.4039), 'test': tensor(2.4723)}\n",
      "step:34500, {'train': tensor(2.4239), 'test': tensor(2.5039)}\n",
      "step:34750, {'train': tensor(2.4451), 'test': tensor(2.4692)}\n",
      "step:35000, {'train': tensor(2.4477), 'test': tensor(2.4760)}\n",
      "step:35250, {'train': tensor(2.4365), 'test': tensor(2.4779)}\n",
      "step:35500, {'train': tensor(2.4261), 'test': tensor(2.4853)}\n",
      "step:35750, {'train': tensor(2.4190), 'test': tensor(2.4712)}\n",
      "step:36000, {'train': tensor(2.4487), 'test': tensor(2.5018)}\n",
      "step:36250, {'train': tensor(2.4018), 'test': tensor(2.4972)}\n",
      "step:36500, {'train': tensor(2.4363), 'test': tensor(2.5003)}\n",
      "step:36750, {'train': tensor(2.4332), 'test': tensor(2.4728)}\n",
      "step:37000, {'train': tensor(2.4279), 'test': tensor(2.4695)}\n",
      "step:37250, {'train': tensor(2.4628), 'test': tensor(2.4864)}\n",
      "step:37500, {'train': tensor(2.4201), 'test': tensor(2.4722)}\n",
      "step:37750, {'train': tensor(2.3997), 'test': tensor(2.4841)}\n",
      "step:38000, {'train': tensor(2.4201), 'test': tensor(2.4415)}\n",
      "step:38250, {'train': tensor(2.4187), 'test': tensor(2.4631)}\n",
      "step:38500, {'train': tensor(2.4140), 'test': tensor(2.4614)}\n",
      "step:38750, {'train': tensor(2.4379), 'test': tensor(2.4735)}\n",
      "step:39000, {'train': tensor(2.3987), 'test': tensor(2.4876)}\n",
      "step:39250, {'train': tensor(2.4167), 'test': tensor(2.4634)}\n",
      "step:39500, {'train': tensor(2.4030), 'test': tensor(2.4661)}\n",
      "step:39750, {'train': tensor(2.4486), 'test': tensor(2.4803)}\n",
      "step:40000, {'train': tensor(2.4163), 'test': tensor(2.4572)}\n",
      "step:40250, {'train': tensor(2.4460), 'test': tensor(2.4797)}\n",
      "step:40500, {'train': tensor(2.4299), 'test': tensor(2.4832)}\n",
      "step:40750, {'train': tensor(2.4439), 'test': tensor(2.4876)}\n",
      "step:41000, {'train': tensor(2.4070), 'test': tensor(2.4715)}\n",
      "step:41250, {'train': tensor(2.4064), 'test': tensor(2.4609)}\n",
      "step:41500, {'train': tensor(2.4047), 'test': tensor(2.4737)}\n",
      "step:41750, {'train': tensor(2.4304), 'test': tensor(2.4842)}\n",
      "step:42000, {'train': tensor(2.4355), 'test': tensor(2.4637)}\n",
      "step:42250, {'train': tensor(2.3959), 'test': tensor(2.4946)}\n",
      "step:42500, {'train': tensor(2.4151), 'test': tensor(2.4579)}\n",
      "step:42750, {'train': tensor(2.4090), 'test': tensor(2.4796)}\n",
      "step:43000, {'train': tensor(2.4112), 'test': tensor(2.4975)}\n",
      "step:43250, {'train': tensor(2.4161), 'test': tensor(2.4793)}\n",
      "step:43500, {'train': tensor(2.4500), 'test': tensor(2.4722)}\n",
      "step:43750, {'train': tensor(2.4130), 'test': tensor(2.4944)}\n",
      "step:44000, {'train': tensor(2.4068), 'test': tensor(2.5081)}\n",
      "step:44250, {'train': tensor(2.4024), 'test': tensor(2.4968)}\n",
      "step:44500, {'train': tensor(2.4050), 'test': tensor(2.4730)}\n",
      "step:44750, {'train': tensor(2.4310), 'test': tensor(2.4724)}\n",
      "step:45000, {'train': tensor(2.4348), 'test': tensor(2.4507)}\n",
      "step:45250, {'train': tensor(2.4571), 'test': tensor(2.4999)}\n",
      "step:45500, {'train': tensor(2.4194), 'test': tensor(2.4551)}\n",
      "step:45750, {'train': tensor(2.3910), 'test': tensor(2.4695)}\n",
      "step:46000, {'train': tensor(2.4115), 'test': tensor(2.4727)}\n",
      "step:46250, {'train': tensor(2.4266), 'test': tensor(2.4651)}\n",
      "step:46500, {'train': tensor(2.4133), 'test': tensor(2.4779)}\n",
      "step:46750, {'train': tensor(2.4344), 'test': tensor(2.4690)}\n",
      "step:47000, {'train': tensor(2.4364), 'test': tensor(2.4554)}\n",
      "step:47250, {'train': tensor(2.4182), 'test': tensor(2.4750)}\n",
      "step:47500, {'train': tensor(2.4247), 'test': tensor(2.4702)}\n",
      "step:47750, {'train': tensor(2.4312), 'test': tensor(2.4750)}\n",
      "step:48000, {'train': tensor(2.4407), 'test': tensor(2.4895)}\n",
      "step:48250, {'train': tensor(2.4109), 'test': tensor(2.4699)}\n",
      "step:48500, {'train': tensor(2.3977), 'test': tensor(2.4789)}\n",
      "step:48750, {'train': tensor(2.4311), 'test': tensor(2.4725)}\n",
      "step:49000, {'train': tensor(2.4317), 'test': tensor(2.4674)}\n",
      "step:49250, {'train': tensor(2.4560), 'test': tensor(2.4807)}\n",
      "step:49500, {'train': tensor(2.4283), 'test': tensor(2.4847)}\n",
      "step:49750, {'train': tensor(2.4130), 'test': tensor(2.4636)}\n",
      "step:50000, {'train': tensor(2.4459), 'test': tensor(2.4231)}\n",
      "step:50250, {'train': tensor(2.4074), 'test': tensor(2.4716)}\n",
      "step:50500, {'train': tensor(2.4279), 'test': tensor(2.4797)}\n",
      "step:50750, {'train': tensor(2.4206), 'test': tensor(2.4879)}\n",
      "step:51000, {'train': tensor(2.4313), 'test': tensor(2.4563)}\n",
      "step:51250, {'train': tensor(2.4193), 'test': tensor(2.4659)}\n",
      "step:51500, {'train': tensor(2.4165), 'test': tensor(2.4779)}\n",
      "step:51750, {'train': tensor(2.4226), 'test': tensor(2.4714)}\n",
      "step:52000, {'train': tensor(2.4236), 'test': tensor(2.4911)}\n",
      "step:52250, {'train': tensor(2.4566), 'test': tensor(2.4635)}\n",
      "step:52500, {'train': tensor(2.4159), 'test': tensor(2.4961)}\n",
      "step:52750, {'train': tensor(2.4144), 'test': tensor(2.5127)}\n",
      "step:53000, {'train': tensor(2.4262), 'test': tensor(2.4527)}\n",
      "step:53250, {'train': tensor(2.4219), 'test': tensor(2.4627)}\n",
      "step:53500, {'train': tensor(2.3986), 'test': tensor(2.4645)}\n",
      "step:53750, {'train': tensor(2.4286), 'test': tensor(2.4646)}\n",
      "step:54000, {'train': tensor(2.4236), 'test': tensor(2.4884)}\n",
      "step:54250, {'train': tensor(2.4302), 'test': tensor(2.4673)}\n",
      "step:54500, {'train': tensor(2.4485), 'test': tensor(2.4876)}\n",
      "step:54750, {'train': tensor(2.4477), 'test': tensor(2.4721)}\n",
      "step:55000, {'train': tensor(2.4143), 'test': tensor(2.4603)}\n",
      "step:55250, {'train': tensor(2.4329), 'test': tensor(2.4744)}\n",
      "step:55500, {'train': tensor(2.4120), 'test': tensor(2.4855)}\n",
      "step:55750, {'train': tensor(2.4408), 'test': tensor(2.4855)}\n",
      "step:56000, {'train': tensor(2.4360), 'test': tensor(2.4774)}\n",
      "step:56250, {'train': tensor(2.4204), 'test': tensor(2.4542)}\n",
      "step:56500, {'train': tensor(2.4198), 'test': tensor(2.4693)}\n",
      "step:56750, {'train': tensor(2.4454), 'test': tensor(2.4626)}\n",
      "step:57000, {'train': tensor(2.4245), 'test': tensor(2.4665)}\n",
      "step:57250, {'train': tensor(2.4104), 'test': tensor(2.4789)}\n",
      "step:57500, {'train': tensor(2.4412), 'test': tensor(2.4740)}\n",
      "step:57750, {'train': tensor(2.4510), 'test': tensor(2.4628)}\n",
      "step:58000, {'train': tensor(2.4251), 'test': tensor(2.4625)}\n",
      "step:58250, {'train': tensor(2.4348), 'test': tensor(2.4732)}\n",
      "step:58500, {'train': tensor(2.4177), 'test': tensor(2.4330)}\n",
      "step:58750, {'train': tensor(2.4248), 'test': tensor(2.4736)}\n",
      "step:59000, {'train': tensor(2.4213), 'test': tensor(2.5124)}\n",
      "step:59250, {'train': tensor(2.4338), 'test': tensor(2.4846)}\n",
      "step:59500, {'train': tensor(2.4285), 'test': tensor(2.4783)}\n",
      "step:59750, {'train': tensor(2.4253), 'test': tensor(2.4886)}\n",
      "tensor(2.3741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "2.374143600463867\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for iter in range(max_iters):\n",
    "    if iter%eval_iters==0:\n",
    "        losses=estimate_loss()\n",
    "        print(f'step:{iter}, {losses}')\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = model.forward(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21c35e20-6e83-4543-a507-4c779cf657e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The wheeanathe arathimuinouthes thuco rd By inousthupthiro thefowasakseyouin's aimeaked tid wond \"Bug wenerisileatus am tetaret wed tooug lloin sthe wind ous d.\"A a r-f\n",
      "I ca icor, ty. te wrsknd useie fe an wioutou thit igheen  theboorutcly bred ated\n",
      "Zeedes thinked Mrves st  m ug the  an sng. er andrdind\n",
      "ghede Thisnt geililk wof r cag f waincound t, m ake eve are on ffrsarakewa ter foul, t, thy\n",
      "\" lendratherfopy\n",
      "\n",
      "\n",
      "Lad t temy rcenclles ley, tlid awhe, thanoure walt.\n",
      "ahet orof sllly ore t\n",
      "t p stouth\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232835e-5c67-45b8-9fb6-e5ebf0099a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
